## transformer核心逻辑图
![[Pasted image 20251229135700.png]]

## 1 Input Embedding
所有的词都是一个512维的向量，这个向量里面具体的内容是需要被训练的，最开始的时候可以认为是随机的小数字，随机分配大概在[-5,5]之间左右的一个正太分布（**工程经验，没有为什么**）。当然有一个字典来对应两者的关系。最后整个内容以n* 512的矩阵输入（n个token，每个字512维）
**这个向量是很重要的,词向量之间的点积就表示了两者之间的关系**
## 2 Position Encoding
**本质**：“我爱你”和“你爱我”是不一样的，但是输入的时候无法区分。所以为了体现出这种先后顺序，我们需要给输入加上一个底噪，让它蕴含这样的信息
$$ PE(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d}}})$$
$$ PE(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d}}})$$
**关键问题**：
* 为什么是sin：因为sin保证了PE的值在0-1之间，这样作为底噪不会太大，但也保证了有。同时这个底噪。
* d是多少：就是512，是输入向量的维度
* $pos$和$i$ 分别代表的是行数，和512维里面每个值。$0<pos<token总数$；$0<=i<=512$
**计算结果**：
$$X=X+PE$$
PE的计算和X无关，就是一个固定的值的向量。通过叠加的方式，让X获得了上下文的底噪。之所以不用乘法，是因为我们是添加底噪，如果是乘，那X的部分内容会变得很小，丢失了输入本身的信息，反而违背了初衷。

## 3 Muti-Head Attention（核心关键）
**本质**：除了前后顺序的底噪，我们需要让输入的文本作用于彼此，让模型具有理解上下文的能力。但文本彼此的关联可能不止一种，所以我们要通过
**方法**：先将原来的输入X复制三遍，不妨分别叫做Q,K,V。K种的元素表示我自己，Q表示上下文
 让$QK^T$来得到一种作用，softmax把结果压缩成0-1之间，表示作用的效果，最后在乘上V。但是上下文可能会很多种作用方式，所以我们需要先把原来的矩阵拆分乘几块，然后才能得到
 
 
 $$Q=X\cdot W^Q$$
  $$K=X\cdot W^K$$
   $$V=X\cdot W^V$$
这里W是权重矩阵，X是前面的输入，此处的W的权重矩阵最开始都是随机生成的，但是由于后面的计算方法，Q，K，V会渐渐趋向他们各自特有的含义。此处只是做一个简单矩阵运算，并不需要激活函数Relu。
$$ X_{after}=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
* *$d_k$: 取决于你想要研究几个作用方式，此处拆分的 $d_k$ 并非切断了句子（Sequence），而是切分了特征空间（Dimension）。每个头（Head）依然能看到完整的句子，只是关注的角度（如语法、语义、指代）不同。
**核心问题**：
* 1. 为什么 $W^Q, W^K, W^V$ 会“分道扬镳”？
	你可能会问：既然都是线性层，为什么不直接算 $X \cdot X^T$？
	- **打破对称性（Breaking Symmetry）：** 如果没有 $W^Q$ 和 $W^K$，点积矩阵 $X \cdot X^T$ 永远是一个**对称矩阵**。这意味着“A 对 B 的注意力”必须等于“B 对 A 的注意力”。但在语言中，这显然不对（例如：动词“爱”对主语“我”的检索，和“我”对“爱”的检索逻辑完全不同）。
	- **空间投影的解耦：** * $W^Q$ 和 $W^K$ 的存在，相当于把原始向量 $X$ 投影到了两个**不同的特征子空间**。
	    - 通过梯度下降，模型发现：为了减小 Loss，它必须让 $W^Q$ 学习如何提取“问题特征”，让 $W^K$ 学习如何提取“匹配标签”。    
	    - **数学动力：** 反向传播时，$\frac{\partial Loss}{\partial W^Q}$ 和 $\frac{\partial Loss}{\partial W^K}$ 的梯度方向是不一样的。Loss 函数要求模型在 $Q \cdot K^T$ 时，正确的词对得分更高。为了达到这个目标，这两个矩阵必须演化出互补的基向量。

*  2. $Q \cdot K^T$ 后的 Softmax 到底是怎么操作的？
	假设我们有 2 个词，每个头 64 维。那么 $Q$ 是 $2 \times 64$，$K^T$ 是 $64 \times 2$。	
	相乘后得到的 $Score$ 矩阵是一个 $2 \times 2$ 的方阵：
	$$\begin{bmatrix} s_{1,1} & s_{1,2} \\ s_{2,1} & s_{2,2} \end{bmatrix}$$
	- $s_{1,2}$ 代表：第 1 个词（Query）对第 2 个词（Key）的原始匹配分。
	- **Softmax 操作：** 是**逐行（Row-wise）**进行的。
	    
	    - 对于第一行 $[s_{1,1}, s_{1,2}]$，执行 $\frac{e^{s_{1,i}}}{\sum e^{s_{1,j}}}$。
	        
	    - 结果变成 $[\alpha_{1,1}, \alpha_{1,2}]$，这两个数加起来等于 1。   
	- **物理意义：** 这行概率告诉你，在生成第 1 个词的特征时，你应该分配百分之多少的注意力给词 1，百分之多少给词 2。
## 4 Feed Forward:
最大的，纯粹的神经网络。前面的多头注意力机制，转换器都是一步的运算量，这个是数百亿神经元的运算网络。不过需要注意的是它并不一定是整个transformer中计算量最大的部分，因为神经网络的时间复杂度是O（n),但是前面的多头注意力机制是O(n^2)。

## 5 Add & Norm
* Add:由于通过梯度下降来进行优化的运算在层数更多的时候容易出现梯度丢失的情况，所以每次运算的时候都叠加之前的运算结果：
$$Output = x + F(x)$$
这样即使前面算出来都是0，也能至少保证输入传递下去
* Norm：将一层的输出，也就是下一层的输入归到一个合理的范围内，不然Add每次加，最后就会变得很大

##  最后的输出：
* output是应该输出的内容，你用来的训练的
* output probability: 输出是某个token的概率，最后我们用的时候就把概率最高的拿来输出。